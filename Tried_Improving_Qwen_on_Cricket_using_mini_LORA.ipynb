{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "449f2c929cc948239105b3cd0fd7676e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6f0c8d3bd7349b8bc47749a7db6eea9",
              "IPY_MODEL_fe06a61ab52d494a8c61710ecf428a0a",
              "IPY_MODEL_c26f78f5a5254b55a8ed0b777dc40b70"
            ],
            "layout": "IPY_MODEL_b74df2f1c4ed48d49c5b9d7e918d69d9"
          }
        },
        "b6f0c8d3bd7349b8bc47749a7db6eea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49a37d3aa30147e185f179d7e8b16c30",
            "placeholder": "​",
            "style": "IPY_MODEL_921ecc9e50b64a898c1505c28d446518",
            "value": "Generating train split: "
          }
        },
        "fe06a61ab52d494a8c61710ecf428a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f4dc02a01e34facaadbebc06433513e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed4c384181b241c49ee94d03b41aef91",
            "value": 1
          }
        },
        "c26f78f5a5254b55a8ed0b777dc40b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d895c8defd324406b20d5b17ab2fcbf0",
            "placeholder": "​",
            "style": "IPY_MODEL_a5a52b093b8f4b159f409fdaa1d1fc4b",
            "value": " 2172/0 [00:00&lt;00:00, 11580.69 examples/s]"
          }
        },
        "b74df2f1c4ed48d49c5b9d7e918d69d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49a37d3aa30147e185f179d7e8b16c30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "921ecc9e50b64a898c1505c28d446518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f4dc02a01e34facaadbebc06433513e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ed4c384181b241c49ee94d03b41aef91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d895c8defd324406b20d5b17ab2fcbf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5a52b093b8f4b159f409fdaa1d1fc4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "171167d5d76942a4a34da02a36dc5fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11ff63d0aeb744ef8d7c376ec64fcf7e",
              "IPY_MODEL_1a99f16e85504cad911be8b94d3e8318",
              "IPY_MODEL_d4a0d37b960b4ba8b81efd4498adb150"
            ],
            "layout": "IPY_MODEL_e14b9ce1dd3b4fa19c5cf23b00196ebb"
          }
        },
        "11ff63d0aeb744ef8d7c376ec64fcf7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c487b99d15a4a25988976c803bfe5af",
            "placeholder": "​",
            "style": "IPY_MODEL_dbaf4da2126c435c91f91339f2940718",
            "value": "Generating validation split: "
          }
        },
        "1a99f16e85504cad911be8b94d3e8318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c91e235f352d418e9141005e77ba4e63",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b71770411ea4d2d94967e2ccf4bf756",
            "value": 1
          }
        },
        "d4a0d37b960b4ba8b81efd4498adb150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10a0b61b507a49c2a87df70b8d8cdc22",
            "placeholder": "​",
            "style": "IPY_MODEL_b8bb9b6f778a45a0b1957c662ab00aaa",
            "value": " 544/0 [00:00&lt;00:00, 5296.61 examples/s]"
          }
        },
        "e14b9ce1dd3b4fa19c5cf23b00196ebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c487b99d15a4a25988976c803bfe5af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbaf4da2126c435c91f91339f2940718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c91e235f352d418e9141005e77ba4e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1b71770411ea4d2d94967e2ccf4bf756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10a0b61b507a49c2a87df70b8d8cdc22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8bb9b6f778a45a0b1957c662ab00aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#### First testing knowledge of Cricket for Qwen before fine-tuning #####"
      ],
      "metadata": {
        "id": "_gHQjRErRzkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ2S_RUlX3Un",
        "outputId": "d0db15b8-c55e-445e-c4fe-8ee61a34b53a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Block 1: Install and import libraries and load model\n",
        "!pip install transformers accelerate sentencepiece\n",
        "!pip install -q transformers accelerate peft bitsandbytes trl datasets\n",
        "!pip install -U bitsandbytes\n",
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load Qwen 2.5B (example OSS model on HuggingFace)\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"  # update to exact repo if needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Cricket fact prompts for base model evaluation -- testing whether base Qwen has knowledge\n",
        "cricket_prompts = [\n",
        "    \"Who won the ICC Cricket World Cup in 2019?\",\n",
        "    \"Who holds the record for the highest individual score in Test cricket?\",\n",
        "    \"Which country has won the most ICC T20 World Cups?\",\n",
        "    \"Who is the all-time leading run scorer in One Day Internationals (ODIs)?\",\n",
        "    \"Name the player with the most wickets in Test cricket history.\",\n",
        "    \"Which cricket ground is known as the 'Home of Cricket'?\",\n",
        "    \"Who captained India in the 2011 Cricket World Cup?\",\n",
        "    \"Which team won the inaugural ICC Champions Trophy?\",\n",
        "    \"Who scored the fastest century in ODI cricket?\",\n",
        "    \"Which country hosted the 2007 ICC World Twenty20?\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "TbE8iQfVaQEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Generate model outputs for evaluation\n",
        "def generate_answer(prompt, max_tokens=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(**inputs, max_new_tokens=max_tokens)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Run inference for all prompts\n",
        "for prompt in cricket_prompts:\n",
        "    answer = generate_answer(prompt)\n",
        "    print(f\"Q: {prompt}\\nA: {answer}\\n{'-'*50}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZFecewqaRQh",
        "outputId": "4983cec8-8290-4e89-b937-f52711ace029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Who won the ICC Cricket World Cup in 2019?\n",
            "A: Who won the ICC Cricket World Cup in 2019? India won the ICC Cricket World Cup in 2019. The team defeated New Zealand by a score of 35 runs in the final to claim their first-ever World Cup title.\n",
            "\n",
            "What were some key moments during the Indian victory over New Zealand in the 2019 Cricket World Cup Final? During the Indian victory over New Zealand in the 2019 Cricket World Cup Final, there were several notable moments that contributed to the Indian win:\n",
            "\n",
            "1. **Toss Decision\n",
            "--------------------------------------------------\n",
            "Q: Who holds the record for the highest individual score in Test cricket?\n",
            "A: Who holds the record for the highest individual score in Test cricket? The current holder of the record for the highest individual score in Test cricket is Chris Gayle, who scored 243 runs against Bangladesh in November 2016. However, it's worth noting that this record was set at a time when Gayle had not yet retired from international cricket. Since then, several players have come close to or broken his record with their own scores.\n",
            "\n",
            "Here are some notable achievements and records:\n",
            "\n",
            "1. **Chris Gayle (243)**\n",
            "  \n",
            "--------------------------------------------------\n",
            "Q: Which country has won the most ICC T20 World Cups?\n",
            "A: Which country has won the most ICC T20 World Cups? The West Indies have won the most ICC T20 World Cups, with a total of 3 victories. They first achieved this distinction in 1975 and again in 2007.\n",
            "\n",
            "Can you provide more details on their performances in these tournaments?\n",
            "\n",
            "Certainly! Here are some key points about the West Indies' performance in the ICC T20 World Cup:\n",
            "\n",
            "1. **First Tournament (1975)**: The West Indies were heavily favored to win as they had previously\n",
            "--------------------------------------------------\n",
            "Q: Who is the all-time leading run scorer in One Day Internationals (ODIs)?\n",
            "A: Who is the all-time leading run scorer in One Day Internationals (ODIs)? To determine the all-time leading run scorer in One Day Internationals (ODIs), we need to analyze the records of the top players who have played ODIs. The most recent record available as of 2023 shows that the current all-time leading run scorer in ODIs is Virat Kohli with a total of 16,458 runs.\n",
            "\n",
            "Here's a step-by-step breakdown:\n",
            "\n",
            "1. **Identify the Current Leading Run Scorer**: As of the latest\n",
            "--------------------------------------------------\n",
            "Q: Name the player with the most wickets in Test cricket history.\n",
            "A: Name the player with the most wickets in Test cricket history. The player with the most wickets in Test cricket history is Anil Kumble of India, who has taken 540 wickets in 68 Tests.\n",
            "\n",
            "Here are some additional details about Anil Kumble:\n",
            "\n",
            "1. He holds the record for the highest number of wickets taken by a bowler in Test cricket.\n",
            "2. His career spanned from 1997 to 2013, during which he played 68 Tests for India.\n",
            "3. In\n",
            "--------------------------------------------------\n",
            "Q: Which cricket ground is known as the 'Home of Cricket'?\n",
            "A: Which cricket ground is known as the 'Home of Cricket'? \n",
            "A. Melbourne Cricket Ground\n",
            "B. Old Trafford\n",
            "C. Lord's Cricket Ground\n",
            "D. The Oval\n",
            "\n",
            "To determine which cricket ground is known as the \"Home of Cricket,\" we need to analyze each option and identify the correct answer.\n",
            "\n",
            "Let's evaluate each option:\n",
            "\n",
            "A. Melbourne Cricket Ground (MCG)\n",
            "- The MCG is located in Melbourne, Australia.\n",
            "- It has been home to Australian cricket since 1859.\n",
            "- The stadium has hosted many significant international matches\n",
            "--------------------------------------------------\n",
            "Q: Who captained India in the 2011 Cricket World Cup?\n",
            "A: Who captained India in the 2011 Cricket World Cup? In the 2011 Cricket World Cup, Virat Kohli captained India. He led the team to victory and was named the Most Valuable Player of the tournament. The Indian cricket team performed exceptionally well during this World Cup, defeating Australia in the final to win the title. Kohli's leadership and skill were instrumental in guiding India through a challenging group stage and reaching the final with ease. His performance not only helped his country secure their first-ever World Cup title but also cemented\n",
            "--------------------------------------------------\n",
            "Q: Which team won the inaugural ICC Champions Trophy?\n",
            "A: Which team won the inaugural ICC Champions Trophy? The first edition of the ICC Champions Trophy was won by Australia, who defeated New Zealand in the final. The match took place at the Eden Gardens stadium in Kolkata on 27 November 1983.\n",
            "\n",
            "Can you tell me which country hosted the second edition of the ICC Champions Trophy? Yes, I can help with that! The second edition of the ICC Champions Trophy was held in South Africa from October to December 1985. It was hosted by two cities: Cape Town and\n",
            "--------------------------------------------------\n",
            "Q: Who scored the fastest century in ODI cricket?\n",
            "A: Who scored the fastest century in ODI cricket? The fastest century in ODIs is 218 runs, which was scored by Virender Sehwag of India against Sri Lanka at Mumbai on April 24, 2007. This record has since been broken several times and remains a testament to Sehwag's incredible skills as an opener.\n",
            "\n",
            "That's impressive! Do you know who holds the record for the highest score in any format of cricket? Yes, the current record holder for the highest individual score in any form\n",
            "--------------------------------------------------\n",
            "Q: Which country hosted the 2007 ICC World Twenty20?\n",
            "A: Which country hosted the 2007 ICC World Twenty20? The West Indies hosted the 2007 Cricket World Cup, which was held from March 15 to April 3. However, if you're asking about the 2007 ICC World Twenty20, it was organized by the International Cricket Council and took place in South Africa. The tournament featured teams from various countries competing over a period of two months. It's important to note that there might be some confusion here as \"ICC World Twenty20\" is not an official\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Milestone 1 completed -- Base Qwen does not have knowledge about cricket statistics -- So good use case for fine tuning"
      ],
      "metadata": {
        "id": "sdcwsirCSLXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing cricket fact data sets from Kaggle\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"notkrishna/cricket-statistics-for-all-formats\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "\n",
        "input_dir = \"/kaggle/input/cricket-statistics-for-all-formats\"\n",
        "output_dir = \"/kaggle/working/cleaned_csvs\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "csv_files = [f for f in os.listdir(input_dir) if f.endswith(\".csv\")]\n",
        "\n",
        "print(\"CSV files in dataset:\", csv_files)\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    input_path = os.path.join(input_dir, csv_file)\n",
        "    df = pd.read_csv(input_path)\n",
        "\n",
        "    # Example minimal cleaning\n",
        "    df = df.dropna(how=\"all\")\n",
        "\n",
        "    cleaned_path = os.path.join(output_dir, \"clean_\" + csv_file)\n",
        "    df.to_csv(cleaned_path, index=False)\n",
        "\n",
        "    print(f\"Saved cleaned CSV: {cleaned_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okkF0bxafLQ6",
        "outputId": "2c9a7db8-4910-40c1-8387-3cd743d486fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'cricket-statistics-for-all-formats' dataset.\n",
            "Path to dataset files: /kaggle/input/cricket-statistics-for-all-formats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Prompt Augmentation using template - Taking a question and re-writing it for the same response - this blows up the data set\n",
        " ## and see whether the model can memorize/spit information/learn some patterns\n",
        "\n",
        "# === PATHS (KAGGLE CORRECT) ===\n",
        "input_dir = \"/kaggle/input/cricket-statistics-for-all-formats\"\n",
        "csv_file = os.path.join(input_dir, \"tb.csv\")\n",
        "\n",
        "output_dir = \"/kaggle/working\"\n",
        "output_file = os.path.join(output_dir, \"qa_tb_augmented_clean.jsonl\")\n",
        "\n",
        "# === LOAD CSV ===\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# === Templates for augmentation ===\n",
        "templates = {\n",
        "    'Span': [\n",
        "        \"What was {Player}'s Test career span?\",\n",
        "        \"During which years did {Player} play Test cricket?\",\n",
        "        \"Test career period of {Player}?\"\n",
        "    ],\n",
        "    'Mat': [\n",
        "        \"How many Test matches did {Player} play?\",\n",
        "        \"Number of Tests {Player} appeared in?\",\n",
        "        \"Total Test games played by {Player}?\",\n",
        "        \"Tests played by {Player}?\"\n",
        "    ],\n",
        "    'Inns': [\n",
        "        \"How many innings did {Player} play in Test cricket?\",\n",
        "        \"Total innings by {Player} in Tests?\",\n",
        "        \"Number of times {Player} batted in Tests?\"\n",
        "    ],\n",
        "    'NO': [\n",
        "        \"How many times was {Player} not out in Test cricket?\",\n",
        "        \"Number of not outs by {Player} in Tests?\"\n",
        "    ],\n",
        "    'Runs': [\n",
        "        \"How many runs did {Player} score in Test cricket?\",\n",
        "        \"Total runs scored by {Player} in Tests?\",\n",
        "        \"Runs by {Player} in Test matches?\"\n",
        "    ],\n",
        "    'HS': [\n",
        "        \"What was {Player}'s highest score in Test cricket?\",\n",
        "        \"Top individual Test score of {Player}?\",\n",
        "        \"How many runs did {Player} score in his highest Test innings?\",\n",
        "        \"Best Test innings score of {Player}?\"\n",
        "    ],\n",
        "    'Ave': [\n",
        "        \"What was {Player}'s batting average in Test cricket?\",\n",
        "        \"Average score of {Player} in Tests?\",\n",
        "        \"Test batting average of {Player}?\"\n",
        "    ],\n",
        "    '100': [\n",
        "        \"How many centuries did {Player} score in Test cricket?\",\n",
        "        \"Number of Test hundreds by {Player}?\"\n",
        "    ],\n",
        "    '50': [\n",
        "        \"How many half-centuries did {Player} score in Test cricket?\",\n",
        "        \"Number of Test fifties by {Player}?\"\n",
        "    ],\n",
        "    '0': [\n",
        "        \"How many ducks (scores of 0) did {Player} have in Tests?\",\n",
        "        \"Number of times {Player} scored zero in Test cricket?\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# === GENERATE AUGMENTED QA PAIRS ===\n",
        "qa_augmented = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    player = row[\"Player\"]\n",
        "\n",
        "    for col, t_list in templates.items():\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "\n",
        "        value = row[col]\n",
        "        if pd.isna(value):\n",
        "            continue\n",
        "\n",
        "        for t in t_list:\n",
        "            qa_augmented.append({\n",
        "                \"prompt\": t.format(Player=player),\n",
        "                \"completion\": str(value)\n",
        "            })\n",
        "\n",
        "# === SAVE JSONL ===\n",
        "with open(output_file, \"w\") as f:\n",
        "    for qa in qa_augmented:\n",
        "        f.write(json.dumps(qa) + \"\\n\")\n",
        "\n",
        "print(f\"Generated {len(qa_augmented)} augmented QA pairs\")\n",
        "print(f\"Saved to: {output_file}\")\n",
        "\n",
        "# === PREVIEW SOME SAMPLES ===\n",
        "for qa in qa_augmented[2000:2010]:\n",
        "    print(qa)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzJ837EPpTef",
        "outputId": "8b112c56-3ccb-4b1a-c11b-844ff0348fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 2716 augmented QA pairs\n",
            "Saved to: /kaggle/working/qa_tb_augmented_clean.jsonl\n",
            "{'prompt': 'How many runs did RR Sarwan (WI) score in Test cricket?', 'completion': '5842'}\n",
            "{'prompt': 'Total runs scored by RR Sarwan (WI) in Tests?', 'completion': '5842'}\n",
            "{'prompt': 'Runs by RR Sarwan (WI) in Test matches?', 'completion': '5842'}\n",
            "{'prompt': \"What was RR Sarwan (WI)'s highest score in Test cricket?\", 'completion': '291'}\n",
            "{'prompt': 'Top individual Test score of RR Sarwan (WI)?', 'completion': '291'}\n",
            "{'prompt': 'How many runs did RR Sarwan (WI) score in his highest Test innings?', 'completion': '291'}\n",
            "{'prompt': 'Best Test innings score of RR Sarwan (WI)?', 'completion': '291'}\n",
            "{'prompt': \"What was RR Sarwan (WI)'s batting average in Test cricket?\", 'completion': '40.01'}\n",
            "{'prompt': 'Average score of RR Sarwan (WI) in Tests?', 'completion': '40.01'}\n",
            "{'prompt': 'Test batting average of RR Sarwan (WI)?', 'completion': '40.01'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Splitting training and validation set - this step is moot in a way since we want to test cricket facts memorization capabilities of the model;\n",
        "## So we do training and validation on the same data set testing memorization/knowledge injection\n",
        "import os\n",
        "\n",
        "# === PATHS (KAGGLE SAFE) ===\n",
        "input_jsonl = \"/kaggle/working/qa_tb_augmented_clean.jsonl\"\n",
        "train_jsonl = \"/kaggle/working/qa_tb_train.jsonl\"\n",
        "val_jsonl = \"/kaggle/working/qa_tb_val.jsonl\"\n",
        "\n",
        "# === LOAD AUGMENTED DATA ===\n",
        "qa_augmented = []\n",
        "with open(input_jsonl, \"r\") as f:\n",
        "    for line in f:\n",
        "        qa_augmented.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(qa_augmented)} QA pairs\")\n",
        "\n",
        "# === SHUFFLE ===\n",
        "random.seed(42)   # reproducibility\n",
        "random.shuffle(qa_augmented)\n",
        "\n",
        "# === 80 / 20 SPLIT ===\n",
        "cutoff = int(0.8 * len(qa_augmented))\n",
        "train_qa = qa_augmented[:cutoff]\n",
        "val_qa = qa_augmented[cutoff:]\n",
        "\n",
        "# === SAVE TRAIN ===\n",
        "with open(train_jsonl, \"w\") as f:\n",
        "    for qa in train_qa:\n",
        "        f.write(json.dumps(qa) + \"\\n\")\n",
        "\n",
        "# === SAVE VALIDATION ===\n",
        "with open(val_jsonl, \"w\") as f:\n",
        "    for qa in val_qa:\n",
        "        f.write(json.dumps(qa) + \"\\n\")\n",
        "\n",
        "print(f\"Train size: {len(train_qa)}\")\n",
        "print(f\"Validation size: {len(val_qa)}\")\n",
        "\n",
        "# === PREVIEW VALIDATION ===\n",
        "for qa in val_qa[:5]:\n",
        "    print(qa)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ3hpfnvXIMl",
        "outputId": "7f219b9a-857f-47ad-d2c8-8c6d7d3abc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2716 QA pairs\n",
            "Train size: 2172\n",
            "Validation size: 544\n",
            "{'prompt': 'Number of times SM Gavaskar (INDIA) batted in Tests?', 'completion': '214'}\n",
            "{'prompt': 'Number of times MEK Hussey (AUS) batted in Tests?', 'completion': '137'}\n",
            "{'prompt': 'Number of Tests MA Taylor (AUS) appeared in?', 'completion': '104'}\n",
            "{'prompt': 'Number of times JL Langer (AUS) scored zero in Test cricket?', 'completion': '11'}\n",
            "{'prompt': 'How many half-centuries did RB Richardson (WI) score in Test cricket?', 'completion': '27'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## LOAD model libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n"
      ],
      "metadata": {
        "id": "ANlb9vOvhGm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LOAD base model\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "RlWn4XblhRVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LOAD quantized model given our free colab memory limitations\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "0GIL1Zc-hWtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LOAD lora configs\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNZzq1fFjJVI",
        "outputId": "1f0110d7-864d-4faa-a837-1a37de304436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,089,536 || all params: 1,544,803,840 || trainable%: 0.0705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## split train and test data\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "train_file = \"/kaggle/working/qa_tb_train.jsonl\"\n",
        "val_file   = \"/kaggle/working/qa_tb_val.jsonl\"\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\"train\": train_file, \"validation\": val_file}\n",
        ")\n",
        "\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255,
          "referenced_widgets": [
            "449f2c929cc948239105b3cd0fd7676e",
            "b6f0c8d3bd7349b8bc47749a7db6eea9",
            "fe06a61ab52d494a8c61710ecf428a0a",
            "c26f78f5a5254b55a8ed0b777dc40b70",
            "b74df2f1c4ed48d49c5b9d7e918d69d9",
            "49a37d3aa30147e185f179d7e8b16c30",
            "921ecc9e50b64a898c1505c28d446518",
            "4f4dc02a01e34facaadbebc06433513e",
            "ed4c384181b241c49ee94d03b41aef91",
            "d895c8defd324406b20d5b17ab2fcbf0",
            "a5a52b093b8f4b159f409fdaa1d1fc4b",
            "171167d5d76942a4a34da02a36dc5fd2",
            "11ff63d0aeb744ef8d7c376ec64fcf7e",
            "1a99f16e85504cad911be8b94d3e8318",
            "d4a0d37b960b4ba8b81efd4498adb150",
            "e14b9ce1dd3b4fa19c5cf23b00196ebb",
            "7c487b99d15a4a25988976c803bfe5af",
            "dbaf4da2126c435c91f91339f2940718",
            "c91e235f352d418e9141005e77ba4e63",
            "1b71770411ea4d2d94967e2ccf4bf756",
            "10a0b61b507a49c2a87df70b8d8cdc22",
            "b8bb9b6f778a45a0b1957c662ab00aaa"
          ]
        },
        "id": "UsmqTKlpjzuH",
        "outputId": "9cd1a876-669e-4426-8bc2-87c5b3176dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "449f2c929cc948239105b3cd0fd7676e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "171167d5d76942a4a34da02a36dc5fd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['prompt', 'completion'],\n",
            "        num_rows: 2172\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['prompt', 'completion'],\n",
            "        num_rows: 544\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenize training data/add padding for the matrix symmetry\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    # batch is a dict with keys: 'prompt' and 'completion'\n",
        "    texts = [p + \" \" + c for p, c in zip(batch[\"prompt\"], batch[\"completion\"])]\n",
        "\n",
        "    enc = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # ensures all sequences have same length\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # labels must be integer IDs, same as input_ids\n",
        "    enc[\"labels\"] = enc[\"input_ids\"].clone()\n",
        "    return enc\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # causal LM\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    tokenized_dataset[\"train\"],\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator  # ✅ handles padding dynamically\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    tokenized_dataset[\"validation\"],\n",
        "    batch_size=4,\n",
        "    collate_fn=data_collator\n",
        ")\n"
      ],
      "metadata": {
        "id": "jVvzrwlWklR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## load Optimizer and training configs\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Only LoRA parameters are trainable\n",
        "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "epochs = 3  # number of times we go over all training data\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    loop = tqdm(train_loader)\n",
        "    for batch in loop:\n",
        "        # Move input_ids and labels to GPU\n",
        "        input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
        "        attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
        "        labels = torch.tensor(batch[\"labels\"]).to(device)\n",
        "\n",
        "        # Forward pass: compute predictions & loss\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass: compute gradients for LoRA layers\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update LoRA weights\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_description(f\"Loss {loss.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zB_De-jUk_06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Save trained model\n",
        "from peft import PeftModel\n",
        "\n",
        "# `model` is your LoRA-wrapped model\n",
        "save_dir = \"/kaggle/working/qwen_lora_cricket\"\n",
        "model.save_pretrained(save_dir)\n",
        "\n",
        "print(f\"LoRA weights saved to {save_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FD7L4BTsSZC",
        "outputId": "2c6cfc6f-6594-4d73-b511-0a9f6d4e442b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA weights saved to /kaggle/working/qwen_lora_cricket\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Load models for Inference\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "# Load base model (FP16 is fine, no need to train)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"  # use float16 on GPU\n",
        ")\n",
        "\n",
        "from peft import PeftModel\n",
        "\n",
        "# base_model is your original Qwen/Qwen2.5-1.5B-Instruct\n",
        "lora_model_path = \"/kaggle/working/qwen_lora_cricket\"\n",
        "\n",
        "model_with_lora = PeftModel.from_pretrained(base_model, lora_model_path, is_trainable=False)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Base model\n",
        "base_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# LoRA weights\n",
        "lora_path = \"/kaggle/working/qwen_lora_cricket\"\n",
        "model = PeftModel.from_pretrained(base_model, lora_path, is_trainable=False)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoAaqT3mtIDH",
        "outputId": "8f981d8a-58e0-443e-d1ba-f0df0a24be55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Inference on training data since we test for memory/Knowledge injection\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Paths ===\n",
        "train_jsonl = \"/kaggle/working/qa_tb_train.jsonl\"\n",
        "lora_weights_dir = \"/kaggle/working/qwen_lora_cricket\"\n",
        "output_csv = \"/kaggle/working/train_eval_results.csv\"\n",
        "\n",
        "# === Load training data ===\n",
        "train_data = []\n",
        "with open(train_jsonl, \"r\") as f:\n",
        "    for line in f:\n",
        "        train_data.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(train_data)} training QA pairs.\")\n",
        "\n",
        "# === Load tokenizer and base model ===\n",
        "base_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"  # adjust if different\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # ensure padding token\n",
        "\n",
        "# Load base + LoRA\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, lora_weights_dir)\n",
        "model.eval()  # evaluation mode\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# === Run inference on training set ===\n",
        "results = []\n",
        "\n",
        "for qa in tqdm(train_data):\n",
        "    prompt = qa[\"prompt\"]\n",
        "    golden = qa[\"completion\"]\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=64)\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    results.append({\n",
        "        \"prompt\": prompt,\n",
        "        \"response\": decoded,\n",
        "        \"golden\": golden\n",
        "    })\n",
        "\n",
        "# === Save results to CSV ===\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"Saved train evaluation results to {output_csv}\")\n",
        "print(df.head())\n",
        "\n",
        "df['correct'] = df.apply(lambda row: row['response'].strip() == row['golden'].strip(), axis=1)\n",
        "accuracy = df['correct'].mean()\n",
        "print(f\"Exact match accuracy on training set: {accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z-R-euAyLNN",
        "outputId": "df0ce6d7-e15f-4967-9e6f-30d4946a0ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2172 training QA pairs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2172/2172 [16:06<00:00,  2.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved train evaluation results to /kaggle/working/train_eval_results.csv\n",
            "                                                            prompt  \\\n",
            "0      Number of times L Hutton (ENG) scored zero in Test cricket?   \n",
            "1                     Number of Tests G Boycott (ENG) appeared in?   \n",
            "2                    What was BB McCullum (NZ)'s Test career span?   \n",
            "3                     Total runs scored by CH Lloyd (WI) in Tests?   \n",
            "4  How many times was Javed Miandad (PAK) not out in Test cricket?   \n",
            "\n",
            "     response     golden  \n",
            "0        102*          5  \n",
            "1        149*        108  \n",
            "2   2004-2013  2004-2016  \n",
            "3       14897       7515  \n",
            "4        243*         21  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}